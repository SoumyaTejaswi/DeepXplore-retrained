# -*- coding: utf-8 -*-
"""Result

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nnlo1bCkybY0kfmwStyq-zBGxnOkzr_9

**Importing all the required libraries**
"""

import tensorflow as tf
import numpy as np
import os
import cv2
import tensorflow_datasets as tfds
from collections import defaultdict
import sklearn
import random
import shutil
import time
from imageio import imwrite
import seaborn as sns
import matplotlib.pyplot as plt

"""**Downloading the dataset**"""

import tensorflow_datasets as tfds

# Load MNIST dataset with specified configurations
builder = tfds.builder("mnist")
builder.download_and_prepare()
ds_train = builder.as_dataset(split='train', shuffle_files=True, as_supervised=True)
ds_test = builder.as_dataset(split='test', shuffle_files=True, as_supervised=True)
ds_info = builder.info

"""**Preprocessing the train and test data and mounting the google drive**"""

batch_size = 64
def load_and_preprocess_dataset(dataset, batch_size, cache_dir=None):

    dataset = shuffle_and_batch_dataset(dataset, batch_size)
    dataset = map_preprocess_function(dataset)
    if cache_dir:
        dataset = cache_dataset(dataset, cache_dir)
    dataset = prefetch_dataset(dataset)
    return dataset

def shuffle_and_batch_dataset(dataset, batch_size):

    dataset = dataset.shuffle(ds_info.splits['train'].num_examples)
    dataset = dataset.batch(batch_size)
    return dataset

def map_preprocess_function(dataset):

    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    return dataset

def cache_dataset(dataset, cache_dir):

    dataset = dataset.cache(cache_dir)
    return dataset

def prefetch_dataset(dataset):

    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    return dataset

# Preprocess training dataset
ds_train = load_and_preprocess_dataset(ds_train, batch_size, cache_dir='./traindata')

# Preprocess testing dataset
ds_test = load_and_preprocess_dataset(ds_test, batch_size, cache_dir='./testdata')

from google.colab import drive
drive.mount('/content/drive')

"""**LENET - 1 MODEL**"""

def lenet1(input_tensor):

    filters = [4, 12]
    kernel_sizes = [5, 5]
    strides = [1, 1]
    activations = ['tanh', 'tanh']
    pool_sizes = [None, None]

    x = input_tensor
    for i in range(len(filters)):
        x = tf.keras.layers.Conv2D(filters=filters[i],
                                   kernel_size=kernel_sizes[i],
                                   strides=strides[i],
                                   padding='valid',
                                   activation=activations[i])(x)
        if pool_sizes[i]:
            x = tf.keras.layers.AveragePooling2D(pool_size=pool_sizes[i])(x)

    x = tf.keras.layers.Flatten(name='flatten')(x)
    output_layer = tf.keras.layers.Dense(10, activation='softmax')(x)

    model = tf.keras.models.Model(inputs=input_tensor,
                                  outputs=output_layer,
                                  name='LeNet-1')
    model.compile(optimizer=tf.keras.optimizers.Adam(),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

"""**LENET - 4 MODEL**"""

def lenet4(input_tensor):

    filters = [4, 16]
    kernel_sizes = [5, 5]
    strides = [1, 1]
    activations = ['tanh', 'tanh']
    pool_sizes = [None, None]
    dense_units = [120]

    x = input_tensor
    for i in range(len(filters)):
        x = tf.keras.layers.Conv2D(filters=filters[i],
                                   kernel_size=kernel_sizes[i],
                                   strides=strides[i],
                                   padding='valid',
                                   activation=activations[i])(x)
        if pool_sizes[i]:
            x = tf.keras.layers.AveragePooling2D(pool_size=pool_sizes[i])(x)

    x = tf.keras.layers.Flatten(name='flatten')(x)
    for units in dense_units:
        x = tf.keras.layers.Dense(units, activation='tanh')(x)
    output_layer = tf.keras.layers.Dense(10, activation='softmax')(x)

    model = tf.keras.models.Model(inputs=input_tensor,
                                  outputs=output_layer,
                                  name='LeNet-4')
    model.compile(optimizer=tf.keras.optimizers.Adam(),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

"""**Training LENET - 1**"""

def build_model():

    model = lenet1(tf.keras.Input(shape=(28, 28, 1)))
    return model

def summarize_model(model):

    model.summary()

def train_model(model, train_dataset, test_dataset):

    # Define callback
    checkpointer = tf.keras.callbacks.ModelCheckpoint(
        monitor='val_loss',
        filepath='lenet1.best.hdf5',
        verbose=1,
        save_best_only=True
    )

    # Train the model
    history = model.fit(
        train_dataset,
        epochs=20,
        validation_data=test_dataset,
        verbose=1,
        callbacks=[checkpointer]
    )

    # Plot loss history
    plot_loss_history(history)

    # Evaluate the model
    evaluation = model.evaluate(test_dataset, verbose=1)
    print("Test accuracy:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
model = build_model()
summarize_model(model)
train_model(model, ds_train, ds_test)

"""**Training LENET - 4**"""

def build_model():

    model = lenet4(tf.keras.Input(shape=(28, 28, 1)))
    return model

def summarize_model(model):

    model.summary()

def train_model(model, train_dataset, test_dataset):

    # Define callback
    checkpointer = tf.keras.callbacks.ModelCheckpoint(
        monitor='val_loss',
        filepath='lenet4.best.hdf5',
        verbose=1,
        save_best_only=True
    )

    # Train the model
    history = model.fit(
        train_dataset,
        epochs=20,
        validation_data=test_dataset,
        verbose=1,
        callbacks=[checkpointer]
    )

    # Plot loss history
    plot_loss_history(history)

    # Evaluate the model
    evaluation = model.evaluate(test_dataset, verbose=1)
    print("Test accuracy:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
model = build_model()
summarize_model(model)
train_model(model, ds_train, ds_test)

input_tensor = tf.keras.layers.Input(shape=(28, 28, 1), name = 'input')
model2 = lenet4(input_tensor= input_tensor)
model1 = lenet1(input_tensor= input_tensor)

model2.load_weights('/content/lenet4.best.hdf5')
model1.load_weights('/content/lenet1.best.hdf5')

"""**Retraining MNIST**"""

def load_images(path):
    images = []
    labels = []

    # Iterate over directories in the path
    for directory in os.listdir(path):
        if os.path.isdir(os.path.join(path, directory)):
            # Iterate over files in each directory
            for file in os.listdir(os.path.join(path, directory)):
                if os.path.isfile(os.path.join(path, directory, file)):
                    # Read images and convert to grayscale
                    image_path = os.path.join(path, directory, file)
                    image = cv2.imread(image_path)
                    if image is not None:
                        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                        images.append(grayscale_image)
                        labels.append(int(directory))

    return images, labels

"""**USING EXAMPLES GENERATED FROM TRAINING SET**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/train_set_generated/generated_inputs_blackout')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**Blackout transformation (LENET - 1)**"""

def load_and_evaluate_model1_black(model, test_dataset):
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
    model.summary()
    evaluation = model.evaluate(test_dataset)
    print("Accuracy before retraining:", evaluation)

def train_and_plot_history1_black(model, train_images, train_labels, test_dataset, batch_size, epochs):
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)

def plot_history(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
input_tensor3 = tf.keras.layers.Input(shape=(28, 28, 1), name='input')
model_1 = lenet1(input_tensor=input_tensor3)

load_and_evaluate_model1_black(model_1, ds_test)
train_and_plot_history1_black(model_1, images, labels, ds_test, batch_size=64, epochs=20)
evaluation_after_retraining = model_1.evaluate(ds_test)
print("Accuracy after retraining:", evaluation_after_retraining)

"""**Blackout transformation (LENET - 4)**"""

def load_and_evaluate_model2(model, test_dataset):
    model.load_weights('/content/lenet4.best.hdf5')
    model.summary()
    evaluation = model.evaluate(test_dataset)
    print("Accuracy before retraining:", evaluation)

def train_and_plot_history2(model, train_images, train_labels, test_dataset, batch_size, epochs):
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)

def plot_history(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

input_tensor2 = tf.keras.layers.Input(shape=(28, 28, 1), name='input')
model_2 = lenet4(input_tensor=input_tensor2)

load_and_evaluate_model2(model_2, ds_test)
train_and_plot_history2(model_2, images, labels, ds_test, batch_size=64, epochs=20)
evaluation_after_retraining = model_2.evaluate(ds_test)
print("Accuracy after retraining:", evaluation_after_retraining)

"""**Using Light Transformation**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/train_set_generated/generated_inputs_light')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**LENET - 1**"""

def load_and_train_model1_light(model, train_images, train_labels, test_dataset, batch_size, epochs):
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)
    evaluation_after_retraining = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation_after_retraining)

# Example usage:
load_and_train_model1_light(model_1, images, labels, ds_test, batch_size=64, epochs=20)

"""**LENET - 4**"""

def load_and_train_model2_light(model, train_images, train_labels, test_dataset, batch_size, epochs):
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet4.best.hdf5')
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)
    evaluation_after_retraining = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation_after_retraining)

# Example usage:
load_and_train_model2_light(model_2, images, labels, ds_test, batch_size=64, epochs=20)

"""**Using Occlusion**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/train_set_generated/generated_inputs_occl')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**LENET - 1**"""

def load_and_train_model1_occ(model, train_images, train_labels, test_dataset, batch_size, epochs):
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)
    evaluation_after_retraining = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation_after_retraining)

# Example usage:
load_and_train_model1_occ(model_1, images, labels, ds_test, batch_size=64, epochs=20)

"""**LENET - 4**"""

def load_and_train_model2_occ(model, train_images, train_labels, test_dataset, batch_size, epochs):
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet4.best.hdf5')
    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)
    evaluation_after_retraining = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation_after_retraining)

# Example usage:
load_and_train_model2_occ(model_2, images, labels, ds_test, batch_size=64, epochs=20)

"""**NOW USING EXAMPLES GENERATED FROM TEST SET**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/test_set_generated/generated_inputs_blackout')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**Blackout Transformation (LENET - 1)**"""

def load_pretrained_model(input_tensor):

    model = lenet1(input_tensor=input_tensor)
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
    return model

def evaluate_model(model, test_dataset):

    model.summary()
    evaluation = model.evaluate(test_dataset)
    print("Accuracy before retraining:", evaluation)
    return evaluation

def train_and_plot_history1_black_(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)

def plot_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
input_tensor3 = tf.keras.layers.Input(shape=(28, 28, 1), name='input')
model_3 = load_pretrained_model(input_tensor3)
evaluation_before_retraining = evaluate_model(model_1, ds_test)
train_and_plot_history1_black_(model_1, images, labels, ds_test, batch_size=64, epochs=20)

"""**Blackout Transformation (LENET - 4)**"""

def load_pretrained_model(input_tensor):

    model = lenet4(input_tensor=input_tensor)
    model.load_weights('/content/drive/My Drive/MNIST data/models/lenet4.best.hdf5')
    return model

def evaluate_model(model, test_dataset):

    model.summary()
    evaluation = model.evaluate(test_dataset)
    print("Accuracy before retraining:", evaluation)
    return evaluation

def train_and_plot_history(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)
    plot_history(history)

def plot_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
input_tensor2 = tf.keras.layers.Input(shape=(28, 28, 1), name='input')
model_2 = load_pretrained_model(input_tensor2)
evaluation_before_retraining = evaluate_model(model_2, ds_test)
train_and_plot_history(model_2, images, labels, ds_test, batch_size=64, epochs=20)

"""**Light transformation**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/test_set_generated/generated_inputs_light')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**LENET - 1**"""

def load_pretrained_weights(model, weights_path):

    model.load_weights(weights_path)

def train_and_evaluate_model1_light_(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)

    plot_loss_history(history)

    evaluation = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
load_pretrained_weights(model_1, '/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
train_and_evaluate_model1_light_(model_1, images, labels, ds_test, batch_size=64, epochs=20)

"""**LENET - 4**"""

def load_pretrained_weights(model, weights_path):

    model.load_weights(weights_path)

def train_and_evaluate_model2_light_(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)

    plot_loss_history(history)

    evaluation = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
load_pretrained_weights(model_2, '/content/drive/My Drive/MNIST data/models/lenet4.best.hdf5')
train_and_evaluate_model2_light_(model_2, images, labels, ds_test, batch_size=64, epochs=20)

"""**Occlusion Transformation**"""

images, labels = load_images('/content/drive/My Drive/MNIST data/test_set_generated/generated_inputs_occl')
images, labels = tf.convert_to_tensor(images), tf.one_hot(labels, 10)

"""**LENET - 1**"""

def load_pretrained_weights(model, weights_path):

    model.load_weights(weights_path)

def train_and_evaluate_model1_occ_(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)

    plot_loss_history(history)

    evaluation = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
load_pretrained_weights(model_1, '/content/drive/My Drive/MNIST data/models/lenet1.best.hdf5')
train_and_evaluate_model1_occ_(model_1, images, labels, ds_test, batch_size=64, epochs=20)

"""**LENET - 4**"""

def load_pretrained_weights(model, weights_path):

    model.load_weights(weights_path)

def train_and_evaluate_model(model, train_images, train_labels, test_dataset, batch_size, epochs):

    history = model.fit(tf.divide(tf.cast(train_images, tf.float32), tf.constant(255.0)),
                        train_labels,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=test_dataset)

    plot_loss_history(history)

    evaluation = model.evaluate(test_dataset)
    print("Accuracy after retraining:", evaluation)

def plot_loss_history(history):

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

# Example usage:
load_pretrained_weights(model_2, '/content/drive/My Drive/MNIST data/models/lenet4.best.hdf5')
train_and_evaluate_model(model_2, images, labels, ds_test, batch_size=64, epochs=20)